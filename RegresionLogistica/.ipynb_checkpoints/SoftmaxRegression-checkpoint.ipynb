{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.datasets import mnist\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lectura y Prerocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(float) / 255.\n",
    "x_test = x_test.astype(float) / 255.\n",
    "\n",
    "# extraemos los últimos 10K datos para validación\n",
    "x_train, x_val = x_train[:-10000], x_train[-10000:]\n",
    "y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_val = x_val.reshape([x_val.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "####ONE HOT ENCODE####\n",
    "new_y_train = np.zeros((y_train.shape[0], 10))\n",
    "new_y_train[range(len(y_train)), y_train] = 1\n",
    "\n",
    "new_y_val = np.zeros((y_val.shape[0], 10))\n",
    "new_y_val[range(len(y_val)), y_val] = 1\n",
    "\n",
    "new_y_test = np.zeros((y_test.shape[0], 10))\n",
    "new_y_test[range(len(y_test)), y_test] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "np.save('x_train', x_train)\n",
    "np.save('y_train', new_y_train)\n",
    "np.save('x_val', x_val)\n",
    "np.save('y_val', new_y_val)\n",
    "np.save('x_test', x_test)\n",
    "np.save('y_test', new_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "x_val = np.load('x_val.npy')\n",
    "y_val = np.load('y_val.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "y_test = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualizamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plt.imshow(x_train[randint(0, x_train.shape[0])].reshape((28,28)), cmap='gray')\n",
    "plt.imshow(x_train[2].reshape((28,28)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(y_train[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IMPLEMENTACIÓN DESDE CERO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def stable_softmax(Z):\n",
    "    exps = np.exp(Z - np.max(Z))\n",
    "    return exps / exps.sum(axis=-1,keepdims=True) ##Sumatoria se realiza para cada fila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Derivada de la Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def grad_softmax_crossentropy(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Y: Etiquetas de tamaño (m, c) //m: nro ejemplos, c: nro clases\n",
    "    Y_hat: Predicciones (stable softmax) de tamaño (m, c) //m: nro ejemplos, c: nro clases\n",
    "    \"\"\"\n",
    "    m = Y.shape[0] ## . /len(Y_hat) es el término (1/m)\n",
    "    return 1/m * (Y_hat - Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_with_logits(Z: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Z: La predicción de la última capa sin activar (m, c)\n",
    "    Y: El vector de etiquetas sin one hot encoding (m,). Si es de tamaño (m, c) se extraerá solo\n",
    "    \"\"\"\n",
    "    if len(Y.shape) > 1:\n",
    "        Y = Y.argmax(axis=-1) ## Si está en One Hot Encode, extraer las predicciones y en un vector.\n",
    "    \n",
    "    Z_j = Z[range(len(Z)), Y] ## LA PREDICCIÓN SIN ACTIVAR EN LA COLUMNA DEL \"y\" CORRESPONDIENTE.\n",
    "    loss = - Z_j + np.log(np.sum(np.exp(Z),axis=-1)) ## LOG SOFTMAX CON LA SUMATORI PARA CADA FILA.\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict(theta, x, bias):\n",
    "    return stable_softmax(x @ theta + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "n_classes = 10\n",
    "lr = 0.1\n",
    "\n",
    "θ = np.random.randn(input_size, n_classes) * np.sqrt(2.0/input_size)\n",
    "b = np.zeros(n_classes)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "iters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for it in tqdm(range(iters)):\n",
    "    Z = x_train @ θ + b\n",
    "    \n",
    "    loss = cross_entropy_with_logits(Z, y_train)\n",
    "    train_losses.append(loss.mean())\n",
    "    val_losses.append(cross_entropy_with_logits(x_val@θ+b, y_val).mean())\n",
    "    \n",
    "    y_hat = stable_softmax(Z)\n",
    "    \n",
    "    dZ = grad_softmax_crossentropy(y_hat, y_train)\n",
    "    dθ = x_train.T @ dZ\n",
    "    db = dZ.sum(axis=0)\n",
    "    \n",
    "    θ = θ - lr*dθ\n",
    "    b = b - lr*db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gráfica del error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluación de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(np.argmax(predict(θ, x_test, b), axis=1), np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(np.argmax(predict(θ, x_test, b), axis=1), np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predecimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "e = x_test[randint(0, x_test.shape[0])]\n",
    "e = e.reshape(1, len(e))\n",
    "\n",
    "plt.imshow(e.reshape((28,28)), cmap='gray')\n",
    "plt.show()\n",
    "print(np.argmax(predict(θ, e, b)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(verbose=2, solver='lbfgs', multi_class='multinomial', n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, np.argmax(y_train, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross Validation sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_val_pred = model.predict(x_val)\n",
    "\n",
    "y_val_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_val_pred, np.argmax(y_val, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_val_pred, np.argmax(y_val, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test Set SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test_pred, np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_pred, np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definimos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "m, n = x_train.shape\n",
    "n_c = 10\n",
    "\n",
    "model = LogisticRegression(n, n_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definimos la funcion de costo y el optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "images_train = Variable(torch.from_numpy(x_train.astype(np.float32)).view(-1, 28*28))\n",
    "labels_train = Variable(torch.from_numpy(np.argmax(y_train.astype(np.int64), axis=-1)))\n",
    "\n",
    "images_val = Variable(torch.from_numpy(x_val.astype(np.float32)).view(-1, 28*28))\n",
    "labels_val = Variable(torch.from_numpy(np.argmax(y_val.astype(np.int64), axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def print_metrics(epoch, loss, train_out, train_label, val_out, val_label):\n",
    "    print('Iteración:', epoch+1, '|',\n",
    "          'Costo:', loss.item(), '|', \n",
    "          'Train Acc:', accuracy_score(np.argmax(train_out.detach().numpy(), axis=1), train_label), '|', \n",
    "          'Val Acc:', accuracy_score(np.argmax(val_out.detach().numpy(), axis=1), val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    images_train.cuda()\n",
    "    labels_train.cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images_train)\n",
    "    loss = criterion(outputs, labels_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        val_out = model(images_val)\n",
    "        print_metrics(epoch, loss, outputs, labels_train, val_out, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test Set Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images_test = Variable(torch.from_numpy(x_test.astype(np.float32)).view(-1, 28*28))\n",
    "labels_test = Variable(torch.from_numpy(np.argmax(y_test.astype(np.int64), axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pred = model(images_test)\n",
    "pred = np.argmax(pred.detach().numpy(), axis=1)\n",
    "\n",
    "print(accuracy_score(pred, labels_test))\n",
    "print(confusion_matrix(pred, labels_test))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (machine_learning)",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
